{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Uninstall all conflicting or broken packages\n",
        "!pip uninstall -y torch torchvision torchaudio transformers numpy\n",
        "\n",
        "# Step 2: Reinstall numpy (downgrade to compatible version <2.0)\n",
        "!pip install numpy==1.24.4\n",
        "\n",
        "# Step 3: Install PyTorch that matches your CUDA version (Colab uses CUDA 11.8)\n",
        "!pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu118\n",
        "\n",
        "\n",
        "\n",
        "#!pip install accelerate\n",
        "#!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n"
      ],
      "metadata": {
        "id": "-7yVpNfcZeB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Reinstall transformers\n",
        "!pip install transformers==4.37.2  # Or latest stable if needed"
      ],
      "metadata": {
        "id": "vLapjjvcqnL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y torch torchvision torchaudio transformers numpy accelerate\n",
        "\n",
        "# Step 5: Install PyTorch that matches your CUDA version (Colab uses CUDA 11.8)\n",
        "\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "\n",
        "# Step 6: Install transformers and accelerate\n",
        "\n",
        "#!pip install transformers accelerate"
      ],
      "metadata": {
        "id": "r8bQoEAarBI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re,warnings, gc,torch\n",
        "import torch.nn as nn\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score,f1_score,precision_score,recall_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from transformers import DistilBertForSequenceClassification,DistilBertTokenizerFast, RobertaTokenizerFast, RobertaForSequenceClassification, RobertaModel, AutoModelForTokenClassification\n",
        "from transformers import TrainingArguments, Trainer\n",
        "import os"
      ],
      "metadata": {
        "id": "E9Czy4QOe_3C"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pharm_data = pd.read_csv(\"/content/classification_data.csv\")\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "#Encoding our labels to numbers\n",
        "pharm_data['label_encoded'] = label_encoder.fit_transform(pharm_data['label'])"
      ],
      "metadata": {
        "id": "o-Foj-k4ZsYZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#converting text to lower case because roberta base is case sensitive.\n",
        "pharm_data['text']= pharm_data.text.str.lower()\n",
        "pharm_data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "z-09DHY3e7C8",
        "outputId": "0fb50c05-3889-4160-b903-81fe8e6a3919"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   id                                               text                label  \\\n",
              "0   1   drugz caused severe rashes in some participants.       Adverse Effect   \n",
              "1   2  no significant side effects were observed duri...     Positive Outcome   \n",
              "2   3       participants were monitored every two weeks.  Neutral Observation   \n",
              "3   4  increased liver enzymes were noted post-treatm...       Adverse Effect   \n",
              "4   5  patients were instructed to maintain a food di...  Neutral Observation   \n",
              "\n",
              "   label_encoded  \n",
              "0              0  \n",
              "1              2  \n",
              "2              1  \n",
              "3              0  \n",
              "4              1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ed4e706f-9152-457b-be19-447b92c66edf\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>label_encoded</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>drugz caused severe rashes in some participants.</td>\n",
              "      <td>Adverse Effect</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>no significant side effects were observed duri...</td>\n",
              "      <td>Positive Outcome</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>participants were monitored every two weeks.</td>\n",
              "      <td>Neutral Observation</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>increased liver enzymes were noted post-treatm...</td>\n",
              "      <td>Adverse Effect</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>patients were instructed to maintain a food di...</td>\n",
              "      <td>Neutral Observation</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ed4e706f-9152-457b-be19-447b92c66edf')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ed4e706f-9152-457b-be19-447b92c66edf button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ed4e706f-9152-457b-be19-447b92c66edf');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-d5a318cd-50bd-402a-bab3-10e52b2e3315\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d5a318cd-50bd-402a-bab3-10e52b2e3315')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-d5a318cd-50bd-402a-bab3-10e52b2e3315 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "pharm_data",
              "summary": "{\n  \"name\": \"pharm_data\",\n  \"rows\": 1000,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 288,\n        \"min\": 1,\n        \"max\": 1000,\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          522,\n          738,\n          741\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 15,\n        \"samples\": [\n          \"marked improvement in blood pressure control was achieved with druge.\",\n          \"the patient experienced nausea after taking drugx.\",\n          \"drugz caused severe rashes in some participants.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Adverse Effect\",\n          \"Positive Outcome\",\n          \"Neutral Observation\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label_encoded\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0,\n          2,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# shuffling the data and splitting into training, testing datasts\n",
        "pharm_data = pharm_data.sample(frac=1.0)\n",
        "x_train,x_val,y_train,y_val = train_test_split(pharm_data['text'].tolist(),pharm_data['label_encoded'].tolist(),test_size = 0.01,shuffle=True,random_state = 42)"
      ],
      "metadata": {
        "id": "gGQ-oSBDZsa8"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# garbase collection to free up cache\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bpx4iMms5eF",
        "outputId": "e4d1d0a7-7118-468c-856d-37f6179fce81"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "159"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# again split training dataset to save some original texts for prediction.\n",
        "train,val,train_label,val_label = train_test_split(x_train,y_train,test_size = 0.15,shuffle=True,random_state = 42)"
      ],
      "metadata": {
        "id": "pNlSecZsZ7h6"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmsmqsdUacmo",
        "outputId": "2c6622c2-12e0-4792-a32d-34b657fc32c8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "841"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the inputs and create a dataset class for converting the embeddings to tensor and collect input embeddings, attention masks and labels for each text in the corpus\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')\n",
        "train_encodings = tokenizer(train, truncation=True, padding=True)\n",
        "val_encodings = tokenizer(val, truncation=True, padding=True)\n",
        "\n",
        "\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "\n",
        "class DataPrep(torch.utils.data.Dataset):\n",
        "    def __init__(self, embed, labels):\n",
        "        self.embed = embed\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.embed.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = DataPrep(train_encodings, train_label)\n",
        "val_dataset = DataPrep(val_encodings, val_label)"
      ],
      "metadata": {
        "id": "OjXv5mGeDNTC"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=len(label_encoder.classes_))\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.TOKEN_CLS,  # Token Classification\n",
        "    inference_mode=False,\n",
        "    r=8,              # Low-rank dimension\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kImTVmBW-VLQ",
        "outputId": "9078f551-12a2-47f6-b084-7902de54278e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 887,811 || all params: 125,535,750 || trainable%: 0.7072\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# initiate the model and supply training arguments like learning rate, number of epochs and batch size so that the data will be processed in batches and does not take much time for fine tuning,\n",
        "# After initiating arguments, pass them along with the model to trainer for fine tuning the LLM.\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/tmp/results\",\n",
        "\n",
        "    learning_rate=3e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"/tmp/logs\",\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"no\",\n",
        "    load_best_model_at_end=False,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 963
        },
        "id": "55s2Dl27DtXR",
        "outputId": "d06ba294-b5b3-4509-e1cd-90466c5633b0"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No label_names provided for model class `PeftModelForTokenClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='265' max='265' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [265/265 07:40, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.102700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.096300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.083200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.082500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.085900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.065500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.060100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.035500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>1.003600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.965200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.906300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.845500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.739800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.646100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.548700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.473200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.439500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.403200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.330900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.321900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.259800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>0.250400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>0.237000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>0.243300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.220100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>0.209300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=265, training_loss=0.6697901786498304, metrics={'train_runtime': 464.6775, 'train_samples_per_second': 9.049, 'train_steps_per_second': 0.57, 'total_flos': 32749817899500.0, 'train_loss': 0.6697901786498304, 'epoch': 5.0})"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model's performance of validation dataset.\n",
        "metrics = trainer.evaluate()\n",
        "print(metrics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "yp6EdUm9DyhZ",
        "outputId": "f4e88af2-0bc6-4ea2-ecce-6afb7a735ffa"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10/10 00:06]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.13370844721794128, 'eval_runtime': 7.8279, 'eval_samples_per_second': 19.035, 'eval_steps_per_second': 1.277, 'epoch': 5.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Custome function to predict the label of each text. First tokenize the text, pass the encodings to the same device where model is running and collect the outputs.\n",
        "def predict(text):\n",
        "  with torch.no_grad():\n",
        "    inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "    device = model.device\n",
        "    # Move the input tensors to the same device as the model\n",
        "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    predicted_class = torch.argmax(logits, dim=1).item()\n",
        "  return label_encoder.inverse_transform([predicted_class])[0]\n",
        "\n"
      ],
      "metadata": {
        "id": "CwkDOYwTiNNc"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display outputs.\n",
        "predicted = []\n",
        "for i in range(len(x_val)):\n",
        "  label = predict(x_val[i])\n",
        "  le = label_encoder.transform([label])\n",
        "  predicted.append(le)\n",
        "  print(f\"The text = {x_val[i]}, the predicted label of the text = {label}  , label's encoding = {le[0]}, and the original label = {y_val[i]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9W7p3o9dSPd",
        "outputId": "f597b339-c52f-403a-a9e3-e57efed4a49f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The text = enrollment criteria included age and weight specifications., the predicted label of the text = Neutral Observation  , label's encoding = 1, and the original label = 1\n",
            "The text = the patient experienced nausea after taking drugx., the predicted label of the text = Adverse Effect  , label's encoding = 0, and the original label = 0\n",
            "The text = patients were instructed to maintain a food diary., the predicted label of the text = Neutral Observation  , label's encoding = 1, and the original label = 1\n",
            "The text = marked improvement in blood pressure control was achieved with druge., the predicted label of the text = Positive Outcome  , label's encoding = 2, and the original label = 2\n",
            "The text = severe allergic reactions were observed following drugb administration., the predicted label of the text = Adverse Effect  , label's encoding = 0, and the original label = 0\n",
            "The text = severe allergic reactions were observed following drugb administration., the predicted label of the text = Adverse Effect  , label's encoding = 0, and the original label = 0\n",
            "The text = the treatment resulted in full remission for the majority of patients., the predicted label of the text = Positive Outcome  , label's encoding = 2, and the original label = 2\n",
            "The text = mild headaches were reported after the second dose of drugy., the predicted label of the text = Adverse Effect  , label's encoding = 0, and the original label = 0\n",
            "The text = increased liver enzymes were noted post-treatment with druga., the predicted label of the text = Adverse Effect  , label's encoding = 0, and the original label = 0\n",
            "The text = the treatment resulted in full remission for the majority of patients., the predicted label of the text = Positive Outcome  , label's encoding = 2, and the original label = 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluation metrics\n",
        "print(\"accuracy of the classification = \",accuracy_score(y_val,predicted))\n",
        "print(\"F1 score = \",f1_score(y_val,predicted,average='weighted'))\n",
        "print(\"precision score = \",precision_score(y_val,predicted,average='weighted'))\n",
        "print(\"recall score = \",recall_score(y_val,predicted,average='weighted'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMtEpfojEunG",
        "outputId": "66f0cb89-11d8-42d0-bf48-5441850d434d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy of the classification =  1.0\n",
            "F1 score =  1.0\n",
            "precision score =  1.0\n",
            "recall score =  1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Named Entity Recognition"
      ],
      "metadata": {
        "id": "BIUfRho5Hf5B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "#Named Entity Recognition - Same as above, we are reading, splitting and tokenizing the data. However, we need to recognize the specific parts of input rather than classification.\n",
        "# Thus, in data preparation, we are trying to match each label to the word for fine tuning LLM\n",
        "\n",
        "ner_data = pd.read_csv(\"/content/ner_data.csv\")\n",
        "\n",
        "\n",
        "ner_data['tag'].value_counts()\n",
        "\n",
        "# Group data by sentence_id to reconstruct sentences and their tags\n",
        "grouped_data = ner_data.groupby('sentence_id').agg(\n",
        "    sentence=('word', lambda x: ' '.join(x)),\n",
        "    tags=('tag', list)\n",
        ").reset_index()\n",
        "\n",
        "# Use LabelEncoder on the unique tags\n",
        "le = LabelEncoder()\n",
        "# Fit on ALL unique tags including 'O'\n",
        "all_tags = sorted(list(ner_data['tag'].unique()))\n",
        "le.fit(all_tags)\n",
        "\n",
        "# Apply encoding to the grouped tags\n",
        "grouped_data['encoded_tags'] = grouped_data['tags'].apply(lambda x: le.transform(x).tolist())\n",
        "\n",
        "# Split data at the sentence level\n",
        "train_sentences, val_sentences, train_tags, val_tags = train_test_split(\n",
        "    grouped_data['sentence'].tolist(),\n",
        "    grouped_data['encoded_tags'].tolist(),\n",
        "    test_size=0.005,\n",
        "    shuffle=True,\n",
        "    random_state=42\n",
        ")\n",
        "actual_train,actual_val,actual_train_label,actual_val_label = train_test_split(train_sentences,train_tags,test_size = 0.15,shuffle=True,random_state = 42)\n",
        "\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')\n",
        "\n",
        "# Tokenize the sentences and align labels\n",
        "def tokenize_and_align_labels(sentences, tags, tokenizer):\n",
        "    tokenized_inputs = tokenizer(sentences, max_length=50, truncation=True, padding=True, is_split_into_words=False,return_offsets_mapping=True)\n",
        "\n",
        "    labels = []\n",
        "    for i, label in enumerate(tags):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        # Initialize with -100 for all tokens\n",
        "        label_ids = [-100] * len(word_ids)\n",
        "\n",
        "        previous_word_idx = None\n",
        "        for token_idx, word_idx in enumerate(word_ids):\n",
        "            if word_idx is not None:\n",
        "                # Only label the first token of a word\n",
        "                if word_idx != previous_word_idx:\n",
        "\n",
        "                    if word_idx < len(label):\n",
        "                         label_ids[token_idx] = label[word_idx]\n",
        "                    else:\n",
        "                         pass # or set to -100, or the O tag if available\n",
        "\n",
        "\n",
        "            previous_word_idx = word_idx\n",
        "\n",
        "        labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "\n",
        "train_encodings_ner = tokenize_and_align_labels(actual_train, actual_train_label, tokenizer)\n",
        "val_encodings_ner = tokenize_and_align_labels(actual_val, actual_val_label, tokenizer)\n",
        "\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "\n",
        "class DataPrepNER(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        item = {}\n",
        "        for key, val in self.encodings.items():\n",
        "             if isinstance(val[idx], list):\n",
        "                 item[key] = torch.tensor(val[idx])\n",
        "             else:\n",
        "                 item[key] = val[idx]\n",
        "\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings[\"input_ids\"])\n",
        "\n",
        "\n",
        "ner_train_df = DataPrepNER(train_encodings_ner)\n",
        "ner_val_df = DataPrepNER(val_encodings_ner)\n",
        "\n",
        "\n",
        "auto_model = AutoModelForTokenClassification.from_pretrained(\"roberta-base\", num_labels=len(le.classes_))\n",
        "# Pass id2label and label2id to the model config\n",
        "id2label = {i: tag for i, tag in enumerate(all_tags)}\n",
        "label2id = {tag: i for i, tag in enumerate(all_tags)}\n",
        "\n",
        "auto_model = AutoModelForTokenClassification.from_pretrained(\n",
        "    \"roberta-base\",\n",
        "    num_labels=len(le.classes_),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n",
        "\n",
        "\n",
        "ner_train_args = TrainingArguments(\n",
        "    output_dir=\"/tmp/results\",\n",
        "    learning_rate=3e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=1,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"/tmp/logs\",\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"no\",\n",
        "    load_best_model_at_end=False,\n",
        "    # Add evaluation strategy\n",
        "\n",
        ")\n",
        "ner_train = Trainer(\n",
        "    model = auto_model,\n",
        "    args = ner_train_args,\n",
        "    train_dataset = ner_train_df,\n",
        "    eval_dataset = ner_val_df\n",
        ")\n",
        "ner_train.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "1m_O0pcuHp3L",
        "outputId": "4d2bfb18-ca7f-4b5d-d687-4d105c16d541"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='53' max='53' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [53/53 00:04, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.529500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.107100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.013600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.002500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.001700</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=53, training_loss=0.1235515240070252, metrics={'train_runtime': 4.8888, 'train_samples_per_second': 172.844, 'train_steps_per_second': 10.841, 'total_flos': 6899929863840.0, 'train_loss': 0.1235515240070252, 'epoch': 1.0})"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating Model on the validation dataset..\n",
        "ner_train.evaluate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "c4TyMhYjm78K",
        "outputId": "a74b7700-c260-45b7-d889-fc9e51d0dfd3"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10/10 00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 0.0005763536901213229,\n",
              " 'eval_runtime': 0.1512,\n",
              " 'eval_samples_per_second': 992.032,\n",
              " 'eval_steps_per_second': 66.135,\n",
              " 'epoch': 1.0}"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Custome function to predict the label of each word\n",
        "def ner_predict(text):\n",
        "  with torch.no_grad():\n",
        "    inputs = tokenizer(text, max_length = \"max_length\",padding=True, truncation=True, return_tensors=\"pt\")\n",
        "    device = auto_model.device\n",
        "    # Move the input tensors to the same device as the model\n",
        "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
        "    outputs = auto_model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    return logits"
      ],
      "metadata": {
        "id": "mqTtzaTg_HGz"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trying to tokenize the input text and send the embeddings to the same device where model is running. Further, we are trying to get the label of each word in the input sentence.\n",
        "def word_label_predict(text):\n",
        "  inputs = tokenizer(text, return_tensors=\"pt\", return_offsets_mapping=True, truncation=True)\n",
        "  offsets = inputs.pop(\"offset_mapping\")  # Not needed for model but needed for word alignment\n",
        "  device = auto_model.device\n",
        "  # Move the input tensors to the same device as the model\n",
        "  inputs = {key: val.to(device) for key, val in inputs.items()}\n",
        "  # Run model\n",
        "  with torch.no_grad():\n",
        "      outputs = auto_model(**inputs)\n",
        "\n",
        "      predictions = torch.argmax(outputs.logits, dim=2)[0].tolist()\n",
        "\n",
        "  # Convert token predictions to labels\n",
        "  predicted_labels = le.inverse_transform(predictions)\n",
        "\n",
        "  # Align tokens with original words\n",
        "  tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "  word_mapping = tokenizer(text, return_offsets_mapping=True)[\"offset_mapping\"]\n",
        "  print(\"word tokens = \",tokens)\n",
        "  print(\"word mapping = \",word_mapping)\n",
        "  print(\"predicted labels = \",predicted_labels)\n",
        "  h_m = {}\n",
        "  for token, offset, label in zip(tokens, word_mapping, predicted_labels):\n",
        "      if token.startswith(\"\"):  # Roberta adds \"\" to indicate word starts\n",
        "          token = token[1:]\n",
        "      if offset != (0, 0):  # Skip special tokens like <s>, </s>\n",
        "          word = text[offset[0]:offset[1]]\n",
        "          h_m[word] = label\n",
        "  return h_m\n",
        ""
      ],
      "metadata": {
        "id": "dPVA5kWPnMFj"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The outputs represents word tokens, the tokenized indexes (word mapping) and predicted label for each tokenized word.\n",
        "for i in val_sentences:\n",
        "  dictionary = word_label_predict(i)\n",
        "\n",
        "  print(f\"The sentence = {i}\")\n",
        "  for key in dictionary:\n",
        "    print(f\"{key} : {dictionary[key]}\")\n",
        "  print(\"-------------------------------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqvBBDZcscWc",
        "outputId": "f581ec0f-d968-4f7e-fed2-f5dfa77245dc"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word tokens =  ['<s>', 'Pat', 'ients', 'experienced', 'dizz', 'iness', 'after', 'taking', '500', 'mg', 'of', 'Drug', 'A', '</s>']\n",
            "word mapping =  [(0, 0), (0, 3), (3, 8), (9, 20), (21, 25), (25, 30), (31, 36), (37, 43), (44, 47), (47, 49), (50, 52), (53, 57), (57, 58), (0, 0)]\n",
            "predicted labels =  ['O' 'O' 'O' 'O' 'B-SYMPTOM' 'B-SYMPTOM' 'O' 'O' 'B-DOSAGE' 'O' 'O' 'O'\n",
            " 'O' 'O']\n",
            "The sentence = Patients experienced dizziness after taking 500mg of DrugA\n",
            "Pat : O\n",
            "ients : O\n",
            "experienced : O\n",
            "dizz : B-SYMPTOM\n",
            "iness : B-SYMPTOM\n",
            "after : O\n",
            "taking : O\n",
            "500 : B-DOSAGE\n",
            "mg : O\n",
            "of : O\n",
            "Drug : O\n",
            "A : O\n",
            "-------------------------------------------------\n",
            "word tokens =  ['<s>', 'Pat', 'ients', 'experienced', 'nausea', 'after', 'administration', 'of', '500', 'mg', 'of', 'Drug', 'B', '</s>']\n",
            "word mapping =  [(0, 0), (0, 3), (3, 8), (9, 20), (21, 27), (28, 33), (34, 48), (49, 51), (52, 55), (55, 57), (58, 60), (61, 65), (65, 66), (0, 0)]\n",
            "predicted labels =  ['O' 'O' 'O' 'O' 'B-SYMPTOM' 'O' 'O' 'O' 'B-DOSAGE' 'O' 'O' 'O' 'O' 'O']\n",
            "The sentence = Patients experienced nausea after administration of 500mg of DrugB\n",
            "Pat : O\n",
            "ients : O\n",
            "experienced : O\n",
            "nausea : B-SYMPTOM\n",
            "after : O\n",
            "administration : O\n",
            "of : O\n",
            "500 : B-DOSAGE\n",
            "mg : O\n",
            "Drug : O\n",
            "B : O\n",
            "-------------------------------------------------\n",
            "word tokens =  ['<s>', 'Pat', 'ients', 'experienced', 'headache', 'following', '50', 'mg', 'of', 'Drug', 'C', '</s>']\n",
            "word mapping =  [(0, 0), (0, 3), (3, 8), (9, 20), (21, 29), (30, 39), (40, 42), (42, 44), (45, 47), (48, 52), (52, 53), (0, 0)]\n",
            "predicted labels =  ['O' 'O' 'O' 'O' 'B-SYMPTOM' 'O' 'B-DOSAGE' 'O' 'O' 'O' 'O' 'O']\n",
            "The sentence = Patients experienced headache following 50mg of DrugC\n",
            "Pat : O\n",
            "ients : O\n",
            "experienced : O\n",
            "headache : B-SYMPTOM\n",
            "following : O\n",
            "50 : B-DOSAGE\n",
            "mg : O\n",
            "of : O\n",
            "Drug : O\n",
            "C : O\n",
            "-------------------------------------------------\n",
            "word tokens =  ['<s>', 'Pat', 'ients', 'experienced', 'fatigue', 'after', 'taking', '50', 'mg', 'of', 'Drug', 'A', '</s>']\n",
            "word mapping =  [(0, 0), (0, 3), (3, 8), (9, 20), (21, 28), (29, 34), (35, 41), (42, 44), (44, 46), (47, 49), (50, 54), (54, 55), (0, 0)]\n",
            "predicted labels =  ['O' 'O' 'O' 'O' 'B-SYMPTOM' 'O' 'O' 'B-DOSAGE' 'O' 'O' 'O' 'O' 'O']\n",
            "The sentence = Patients experienced fatigue after taking 50mg of DrugA\n",
            "Pat : O\n",
            "ients : O\n",
            "experienced : O\n",
            "fatigue : B-SYMPTOM\n",
            "after : O\n",
            "taking : O\n",
            "50 : B-DOSAGE\n",
            "mg : O\n",
            "of : O\n",
            "Drug : O\n",
            "A : O\n",
            "-------------------------------------------------\n",
            "word tokens =  ['<s>', 'Pat', 'ients', 'experienced', 'cough', 'after', 'taking', '50', 'mg', 'of', 'Drug', 'A', '</s>']\n",
            "word mapping =  [(0, 0), (0, 3), (3, 8), (9, 20), (21, 26), (27, 32), (33, 39), (40, 42), (42, 44), (45, 47), (48, 52), (52, 53), (0, 0)]\n",
            "predicted labels =  ['O' 'O' 'O' 'O' 'B-SYMPTOM' 'O' 'O' 'B-DOSAGE' 'O' 'O' 'O' 'O' 'O']\n",
            "The sentence = Patients experienced cough after taking 50mg of DrugA\n",
            "Pat : O\n",
            "ients : O\n",
            "experienced : O\n",
            "cough : B-SYMPTOM\n",
            "after : O\n",
            "taking : O\n",
            "50 : B-DOSAGE\n",
            "mg : O\n",
            "of : O\n",
            "Drug : O\n",
            "A : O\n",
            "-------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Transparency*\n",
        "## Google Colab uses AI to automatically generate the code and I took the help of code generated in such way for Named Entity Recognition Task.\n",
        "\n"
      ],
      "metadata": {
        "id": "WrPBx5VEFXk4"
      }
    }
  ]
}